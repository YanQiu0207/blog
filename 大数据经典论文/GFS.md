# The Google File System

[TOC]

## 概述

1. 是什么：一个可伸缩的、分布式的文件系统。

2. 目的：满足 google 数据处理的快速增长需求。

3. 特点：

- 在便宜的商用硬件上提供容错性
- 给大量[^1]客户端提供高聚合的性能

4. 目标：性能、扩展性、可靠性和可用性。与先前的文件系统有什么不同：

- 组件故障是常态，而不是异常。由于机器数据很多，任意时间都有机器不能工作，甚至无法从故障中恢复。因此，持续监控、错误检测、容错和自动恢复必须是系统不可或缺的组成部分。
- 以传统的标准来看，文件是巨大的，数 GB 的文件很常见。设计假设和参数（I/O操作和块大小）需要重新考虑。
- 许多文件的变化是追加新数据，而不是覆盖已存在的数据。在这种访问模式下，追加是性能和原子性保证的重点。
- 通过增加灵活性，协同设计的应用程序和文件系统API使整个系统受益。例如放松一致性模型，原子性的追加操作以使得多个客户端可以并发追加一个文件而不需要额外的同步操作。

## 2. 设计概览

### 2.1 假设

- 系统由许多廉价的商用组件组成，可能经常发生故障。它必须持续地监控自己，探测、容错和从故障中快速地恢复。
- 系统存储一定数量的大文件。我们预计会有几百万个文件，每个文件的大小通常为100 MB或更大。多gb文件是常见的情况，应该有效地管理。小文件必须得到支持，但我们不需要为此进行优化

- 工作负载主要由两种读组成：大的流读取和小的随机读取。在大的流读取过程中，同个客户端的连续操作通常读取文件的一部分连续区域。小的读取则是在任意偏移量读取。如果你的应用对性能很敏感，通常会将小的读取组成一个批次并排序，以使得在文件中稳步前进而不是来回移动。
- 写跟两种读类似。
- 系统为同时追加同一个文件的多个客户端高效地实现明确的语义。

- 高持续带宽比低延迟更重要。

### 2.2 接口

- 提供了熟悉的文件系统接口，包括创建、删除、打开、读取、写入和关闭。
- 支持**快照**（章节3.4）：以很小的代价就可以拷贝一个文件或目录
- 支持**记录追加**（章节3.3）：允许多个客户端同时追加到同一个文件，同时保证每个独立的客户端追加操作是原子性的。

### 2.3 架构

![a704b10af7cef45e9a3e66f496708c02.png](en-resource://database/1321:1)

一个 GFS 集群由一个单点 master 和多个 chunkservers 组成，可以被 多个 clients 访问。。

chunk
文件被切分成固定大小的 chunk，每个 chunk 都有一个不变的，全局唯一的 64 bit 大小的 chunk handle，由 master 在 chunk 创建时生成。
为了可靠性，每个 chunk 被复制到多个服务器上。默认是 3 个副本，但用户为不同 chunk 指定不同的复制等级。

chunkserver
在本地磁盘中以 Linux 文件的形式存储 chunk，通过 chunk handle 和 byte range 读写 chunk

master
* 维护了整个文件系统的元数据，包括命名空间、访问控制信息、文件到块的映射关系和块的位置。
* 控制系统范围的活动，例如 chunk 的租约管理、垃圾回收和迁移。

client
client 与 master 进行元数据操作，但所有承载数据的通信都直接到chunkserver。

cache
client 和 chunkserver 都不缓存文件数据，避免了缓存一致性问题，简化了整个系统的设计。
client 缓存了元数据。(filename, chunk index) --> (chunk hanlde, chunk location)
chunkserver 不需要缓存文件数据，这些数据在被存储为本地文件，所以 Linux buffer cache 已经把频繁访问的数据放到内存中了。

### 2.4 单点 Master

单点设计的好处：极大地简化设计，使得 master 能够使用全局知识执行复杂的 chunk 放置和复制决策。

必须最小化 master 在读写过程中的参与，以避免成为瓶颈。具体做法：client 不会通过 master 读写数据。相反，client 只会询问 master 它应该跟哪个 chunkserver 交互。它缓存这个信息一段时间，并且许多后续操作都是直接和 chunkserver 交互。

一个简单的读：

* 由于 chunk 的大小是固定的，client 可以将应用指定的字节偏移量翻译成 chunk 索引。
* client 将文件名和 chunk 索引发给 master，master 返回相应的 chunk handle 和 chunkservers 副本。client 缓存这部分信息：(文件名，chunk 索引) --> （chunk handle，chunkservers 副本）
* client 通过 chunk handle 和字节范围从 chunkserver 上读取文件内容。后续的读取过程不再需要访问 master，直到缓存信息过期或文件重打开。

> 实际上，client 可以一次获取多个 chunk 的位置。

### 2.5 chunk 大小

chunk size 是 64 MB，比典型的文件系统块大小大多了。

大块的优点：

* 减少客户端与 Master 交互的需要，因为同个 chunk 上的读写请求只需要向 Master 请求一次 chunk 的位置信息。在谷歌的业务场景中，应用大多数情况下都是顺序读写文件的。对于随机读写，chunk 越大，需要缓存的信息就越少。
* chunk 越大，客户端的许多操作就集中在同一个 chunk 上，在一段时间内与某个 chunkserver 保持链接能够减少网络开销。
* 能够减少 Master 存储的元数据大小，让 Master 能够把元数据放到内存中，提高性能。

大块的缺点：
* 如果许多客户端访问同个文件，那么就会成为性能热点
* 空间浪费？（lazy space allocation 又是什么？）

如何解决性能热点问题?

* 客户端
  * 错开访问时间，减少并发量
  * 允许客户端向其它客户端读取数据（流水线传输？），减少并发量
* 服务端
  * 增大副本数量，分散请求 

### 2.6 元数据

master 存储了三种主要的元数据：

1. 文件和 chunk 的命名空间
2. 文件到 chunk 的映射
3. chunk 副本的位置

它们都放在内存中，前 2 种元数据在 master 通过操作日志持久化，即使宕机也不会丢失数据。第 3 种由 master 在启动时向 chunkserver 获取。

#### 2.6.1 内存数据结构

将所有的元数据放在内存

- 优点：
  - master 的所有操作都非常快
  - master 可以简单、高效地定期在后台扫描整个系统的状态，以实现垃圾收集、副本分配和 chunk  迁移
- 缺点：
  - 能存储的数据量受内存大小的限制

> 在内存中存储元数据，我们获得了简单、可靠、性能和灵活。相比这些，增加 master 内存的消耗不值一提

#### 2.6.2 chunk 位置信息

存储在哪里？两种选择：

1. 在 master 上持久化
2. 在 chunkserver 上持久化，master 启动时向所有 chunkserver 拉取。之后，由于 master 控制了所有 chunk 位置的分配和通过心跳监控 chunkserver 的状态，所以它能保证它自己的数据是最新的。

实际选了第 2 种方法，因为它更简单。选第一种，master就必须保证各种情况下都能同步 chunkserver  的最新信息，例如 chunk 加入、离开、重启、故障、改名等。

试图在 master 上维护此信息的一致视图是没有意义的，因为chunkserver上的错误可能会导致块自动消失(例如，磁盘可能会变坏并被禁用)，或者操作员可能会重命名chunkserver。对于在自己的磁盘上保留或不保留哪些块，chunkserver拥有最终决定权。

#### 2.6.3 操作日志

操作日志包含关键元数据变更的历史记录，它是
* 元数据的唯一持久化记录
* 逻辑时间线，定义了并发操作的顺序

如何可靠地存储数据，即使机器故障也不丢失？
采用同步复制，等数据在所有机器上落地成功，才给客户端返回成功。保证所有机器上的数据都是最新的。
- 阻塞问题：只要有一台机器故障，就会阻塞后面的操作。可采用 paxos，raft 等共识算法，只要大于一半的机器还活着，系统就能正常运转。
- 吞吐量问题：将少量日志记录集中到一起发送。

如何持久化数据？

* 要求
  * 宕机不丢数据
  * 宕机后能快速恢复
  * 持久化过程中服务不中断

* 操作日志
  * 缺点：启动速度慢
    * 后台定期对日志进行压缩，减少冗余日志
    * 结合状态，减少需要重放的日志量
  * 优点：每次需要记录的数据量小
* 检查点
  * 优点
    * 启动速度快
  * 缺点
    * 由于状态的数据量大，只能间隔记录，宕机丢失数据多
 
完美的办法：operation log + checkpoint

* 记录
  *  每次数据变更，先记录变更日志，再修改内存
  *  后台定期生成检查点。生成后，之前的操作日志就可以丢弃了。检查点多机备份
* 恢复
  * 先找到最近一个完整的检查点文件，然后恢复。注意，生成检查点时宕机可能导致文件不完整，需要跳过这种文件。
  * 重放检查点之后的操作日志。一般数量比较少。

### 2.7 一致性模型

文件命名空间的修改是原子性的。通过锁保证原子性和正确性。

![59748688c9e93e69b62c397430573837.png](en-resource://database/13507:1)

术语解析：

- **一致性** 不管客户端从哪个副本读取数据，它们总是看到相同的数据。
- **可定义的**: 数据修改后，一是要满足一致性，二是客户端要看到它自己修改的全部内容，即不能出现客户端写完后发现文件中有其它客户端内容的情况。

数据变化后文件区域的状态取决于变化的类型，跟变化是否成功无关，跟变化是否并发无关。

如何保证多个副本的数据相同?

- （3.1节）以相同的顺序应用对数据的修改
- （4.5节）使用版本号机制来发现过期的副本。过期的副本可能是因为宕机期间遗失了一些修改。过期的副本不会被继续修改或返回给客户端。

由于客户端缓存了 chunk 的位置，在信息刷新前，它们可能从过期的副本中读取数据。如果想缩小不一致的时间窗口，有两种办法：一是减少缓存的过期时间；二是数据修改后主动通知客户端失效缓存。此外，由于许多文件都是只能追加的，过期副本通常只是少返回一些数据，而不是返回一些过期的数据。当客户端与 Master 重新通信后，就能获取最新的 chunk 位置信息。

组件故障会损坏或破坏数据。GFS 通过 Master 和 chunkserver 之间的握手信息来识别故障的 chunkserver 和用 chunksum 来探测数据损坏。一旦出现问题，将尽快从有效的副本恢复数据。只有 GFS 的所有副本都遗失了，这个 chunk 才会遗失。即使在这种情况下，副本变成不可用，而不是被损坏。

对应用程序的影响（不太了解）：
* 应该采用追加写入而不是覆盖写，因为更高效，更能应对 client 的故障问题
* 写入数据应包含校验和，用来校验数据是否完成
* 写入数据应包含唯一ID，用来去重或排序。如果文件太大，根据ID排序可能比较困难，那么保证按顺序写入就不需要排序了。

namespace locking guarantees atomicity and correctness (Section 4.1);

(b) using chunkversion numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down (Section 4.5).

detects data corruption by checksumming (Section 5.2).

### 3. 系统交互

基于 Master 尽可能少地参与所有操作这个原则来设计系统。基于这个背景，看看数据修改，原子性追加记录和快照这些操作中，client, master 和 chunkservers 是怎么交互的。

#### 3.1 租约与修改顺序

用租约来维护副本间的一致性修改顺序，以最小化 master 的管理负载。全局修改序是如何确定的？

* 第一层：master 选择的租约授予顺序
* 第二层：在某个租约内，primary 选择的修改顺序。primary 对它收到的所有修改分配一串连续的序列号，然后按照序列号大小来应用修改（包括其它副本）

* By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the networktopology regardless of which chunkserver is the primary. Section 3.2 discusses this further.

primary 写入成功后，再通知副本执行。如果任意一个副本发生错误，primary 会给客户端返回错误，客户端可以重试。注意，失败导致状态不一致。

#### 3.2 数据流

分离控制流和数据流是为了更高效地利用网络。

相比给一个个副本传输数据，采用流水线传输能完全利用每个机器的网络出口带宽，一是避免网络瓶颈和高延迟链接（交换机间的链路通常都是），二是最小化推送数据的延迟。

每次都把数据传给离自己最近的，未接收过数据的副本。如果网络拓扑比较简单，可以通过 IP 地址算出离自己最近的副本。

Without networkcongestion, the ideal elapsed time for transferring B bytes to R replicas is B/T + RL where T is the network throughput and L is latency to transfer bytes between two machines. Our network links are typically 100 Mbps (T), and L is far below 1 ms. Therefore, 1 MB can ideally be distributed in about 80 ms

#### 3.3 原子地追加记录

在传统的写操作中，客户端指定数据被写入的偏移量。对同个区域的并发写入是不可序列化的：这个区域最终包含了来自多个客户端的数据片段。

> 原因：一个文件分为了多个 chunk，每个 chunk 上的写入由各自的 primary 负责，不同 primary 之间并没有协调顺序。

在记录追加操作中，客户端只指定数据，由 GFS 选择偏移量，并保证至少一次将数据原子性地追加到文件中。

在谷歌的业务场景中，记录追加操作被频繁使用：不同机器上的多个客户端并发追加内容到同个文件，例如多生产者/单消费者队列、合并多个客户端的结果等。

追加记录时，如果发现最后一个  chunk 空间不够时，要先填充，再找下一个 chunk。为了避免浪费太多空间，限制一次只能追加 1/4 chunk（即16MB） 大小的数据

> 为什么要填充？可能是要根据偏移量定位到 chunk，否则会错位。 

至少原子性地写入一次：如果写入失败，客户端可以重试。GFS 并不保证 chunk 的所有副本的所有字节都是一样的，只保证数据被作为原子单元至少被写入一次。即一旦写入成功，同个 chunk 的所有副本在某个偏移量的写入是确定的（包含了一致性）。

#### 3.4 快照

快照操作几乎是瞬间复制一个文件或一个目录树，并且尽量不中断进行中的修改。特点：

* 瞬间，说明速度快
* 占用空间小
* 尽量不中断进行中的修改，说明可用性高

用途：

* 快速创建大数据集的分支副本
* 对数据修改前先备份当前状态方便回滚

底层原理：copy-on-write，写时拷贝技术

快照流程：

* 客户端发送快照请求
* masteraster 先撤回 chunk 上的所有租约。确保后续所有写入这些 chunk 都需要问 master chunk 的租约持有者，这给了 master 先创建 chunk 副本的机会
*  master 将操作记录到磁盘，然后对内存应用这个操作：复制源文件或者目录树的元数据，它们仍然指向原来的 chunk

写入流程：

* 客户端向 master 发送写入请求以找到租约持有者
* 如果 master 发现这个 chunk 的引用计数大于 1，就延迟回复客户端。master 会让所有持有 chunk 的 chunkserver 原地创建一个新的 chunk
* master 将租约授予新 chunk 的某个副本，然后回复客户端
* 客户端能正常写入 chunk，无法感知到这个 chunk 是刚复制的

## 4. Master 操作

（1）命名空间管理和锁

许多命名空间上的操作耗时高，例如快照操作，但我们不希望阻塞其它操作。因此，我们允许多个操作并发，然后在命名空间上用锁来保证正确的序列化。

命名空间的特点；

* 没有记录目录下有哪些文件，也不支持对目录/文件建立别名。**这暗示了修改目录下的文件，不需要更改目录的元数据？**
* 用一个查找表表示文件路径到元数据的映射
* 所有的文件/目录名在内存中以前缀树的形式组织，树上的每个节点都有一个读写锁

每个 master 操作需要获取一组锁。例如，如果想操作 /d1/d2/.../dn/leaf，那么需要获取 /d1,/d1/d2,/d1/d2/.../dn 的**读锁**，以及  /d1/d2/.../dn/leaf 上的**读锁或写锁**（取决于你想执行的操作）。

注意，不需要获取 /d1/d2/.../dn 的写锁，因为目录没有相应的结构数据，即增加这个目录下的文件不影响目录的元数据。但是需要获取读锁来保证这个目录不会被删除、重命名和快照（因为读锁会阻塞写锁）。

优点：支持同目录下的并发修改，例如可以并发创建某个目录的文件

由于节点很多，用到锁时再创建对象，用完立刻删除。**我的理解：节省内存？**

一种一致的全局序来获取锁可以避免死锁：首先按命名空间树的层级来排序；然后同层级按字典序来排序。**我的理解：所有锁都按同种顺序获取就不会有死锁问题**

（2）副本放置

chunk 副本的放置策略服务于两个目的：

* 最大化数据可靠性和可用性
* 最大化网络带宽利用

为了实现这两个目的，副本放置在不同机器还不够，这些机器还要分别放置在不同机架，以应对磁盘或机器故障，以及最大化利用网络带宽。怎么理解?

第一点，假设整个机器都被破坏或下线了，例如网络交换器或者电源线路故障，某个 chunk 还有副本存活并可用的。

第二点：某个 chunk 的读流量能充分利用多个机架的聚合带宽。当然，写流量也会经过多个机架。

> 看到第一点时，可能会觉得当年的想法有点保守，现在都是跨地域级别保证可用性了。
> 第二点，感觉适合读多写少的系统。

（3）创建、重复制、重平衡

当 master **创建**一个 chunk 时，它要选择哪里来放置这些初始为空的副本。通常考虑下面几个因素：

* 将新副本放置在磁盘空间利用率比较低的 chunkserver。随着时间的过去，这个策略可以平衡不同 chunkservers 之间的磁盘利用率。
* 限制每个 chunkserver 上最近创建操作的数量。虽然创建操作很快，但是它们通常是因为写入而触发的，这暗示了接下来会有很多写流量。
* 将副本放置在多个机架上。

一旦可用副本的数量下降到用户指定的数量之下时，master 会**重复制**一个 chunk，这可能是因为磁盘坏了、数据被破坏等。每个 chunk 的重复制操作执行有先后顺序，通常考虑下面几个因素：

* 现有副本数与用户指定数量的差距
* 倾向于复制最近活跃的文件的 chunk，而不是最近被删除的文件的 chunk
* 为了减少故障对正在运行中的程序的影响，我们也会优先复制那些会阻塞客户端进程的 chunk

如何重复制：从已有的有效副本拷贝数据

放置策略：跟创建一样

限流：限制克隆操作的数量；限制拷贝的速度

**重平衡**：定期检查现有副本的分布情况，并且移动副本以实现更好的磁盘利用率和负载均衡。

如何重平衡：通常选择转移磁盘空闲空间不足的 chunkserver

（4）垃圾回收

**机制**

**优点**

**缺点**

（5）过期副本删除

**version number**



## 5. 容错与问题诊断

### 5.1 高可用

设计系统时，一个巨大的挑战是如何处理频繁的组件故障。组件的数量和指令让这些问题成为常态而不是例外：我们既不能完全信任机器，也不能完全信任磁盘。组件故障轻则系统不可用，重则损坏数据。

（1）快速恢复：无论 master 和 chunkserver 是怎么终止的，它们能够在几秒内恢复状态和重启。客户端和其它服务器发现请求超时后，可以重连到重启的机器，并重试。由于秒级恢复，这个延迟时间非常短。

（2）chunk 复制：每个 chunk 被复制到不同机架上的多个机器上。如果 chunkserver 下线或数据损坏，master 会克隆还存活的副本来创建一个新副本。

（3）master 复制：操作日志和检查点会被复制到多个机器上。

- backup master：每个状态修改只有刷到 master 本地和其它 backup master 的磁盘上时才会被认为是已提交的。当 master 故障时，会切换到 backup master。客户端通过 master 的别名来获取 IP 地址。如果 master 变更，只需要修改 DNS 上的 IP 映射。
- shadow master：增强了读的可用性。当 master 宕机时，提供只读功能。数据稍微落后 master，跟 master 有几分之一秒的延迟。

### 5.2 数据完整性

（1）问题：由于磁盘数量多，经常遇到磁盘故障导致的数据损坏或丢失问题。

（2）监控：如何探测数据是否已损坏

比较不同 chunkserver 的副本来发现数据损坏的做法不切实际，并且 GFS 的原子写入语义并不保证不同 chunkserver 的副本是完全一致的。因此，每个 chunkserver 必须通过校验和（checksum）来独立验证自己副本的数据是否损坏

（3）实现：

一个 chunk 被切分成多个 64 KB 大小的 blocks。每个 block 有一个 32 bit 大小的 checksum。

对于读取操作，chunkserver 在返回任何数据前都需要校验读取范围内每个 block 的 checksum，这保证了错误的数据不会被传播出去。如果某个 block 的内容跟 checksum 不匹配，chunkserver 会给请求者返回错误，并且向 master 报告。请求者会向其它副本读取，而 master 会从其它 chunkserver 拷贝副本并指示该 chunkserver 删掉旧的坏副本。

checksum 对读取操作的性能影响较小。

- 相比要读取的数据，要读取的 chunksum 数据比较少
- 客户端会进一步在 block 的边界对齐读取范围，减少 checksum 的次数
- checksum 的查找（在内存中）和比较不需要任何 I/O，并且计算通常和 I/O 重叠进行

> 备注：指 CPU 等待I/O时可以进行计算？

对于追加到一个 chunk 末尾的写入，checksum 的计算进行了重度优化，毕竟这种写入方式是我们主要的工作负载。对最后一个块，写入一些数据，就增量更新校验和。直到这个块被填充满，会计算一个新的校验和。

> 这里并不一定理解正确？

对于覆盖写入，我们必须要先验证写入范围内的第一个和最后一个 block 的校验和，再写入，避免一些已损坏的数据重新计算校验和后变为了正常数据。

空闲的时候，chunkserver 能扫描和验证不活跃 chunk 的校验和。这能够让我们探测到那些很少读取，但已经损坏的数据。这能让我们及时修复数据，避免出现 master 认为有效副本数量足够而实际有副本数据已损坏的情况。

### 5.3 诊断工具

待续。

## 6. 性能测量

待续。

## 7. 经验

待续。

## 8. 相关工作


## 9. 结论

- 如何提供容错性
  - 持续不断的监控
  - 复制关键数据
  - 快速、自动的恢复机制
- 如何避免单点 Master 成为性能瓶颈

## 疑问

1. 什么叫做高聚合性能，high aggregate performance
2. 可靠性和可用性的区别。
3. 缓存数据块在客户端失去其吸引力？
4. 一台机器部署多个进程：优点是充分利用硬件资源，减少成本；缺点是：可靠性降低，例如CPU资源分配不均，内存占用过大等。
5. Linux 会在内存中缓存频繁访问的文件数据。
6. GFS 的客户端会在读取数据的过程中，把一些数据缓存了下来，那么它究竟缓存了哪些数据来减少频繁的网络往来？在这个缓存机制上，客户端有可能会读到过时的数据吗？
7. master 的数据都会通过操作日志和 Checkpoints 持久化在硬盘上。但这句话其实不完全正确，每个 chunk 存放在什么 chunkserver 上的这些元数据，master 并不会持久化。读完论文之后，你能说说当 master 重启的时候，怎么重新拿到这个数据吗？

* 一个文件为什么要分为多个 chunk 存储，直接一个 chunk 存储整个文件有什么问题吗？
  * 不能充分利用机器资源。如果一个文件很大，就得找个足够大空间的机器来存储。如果分为多个块，一个服务器放不下，还可以放到其它服务器上。 
  * 可靠性低。一旦机器宕机，整个文件都不能读了。而存储到多个服务器上，除了出问题机器上的部分内容无法读取，读取其它内容不受影响。
  * 性能受限。多台服务器可以同时提供服务，比单台服务器吞吐量更大。

* 流水线传输：充分利用了接收端的转发能力，在更短的时间内将数据传输到每一台机器上。更底层的原理：网络是全双工，接收数据的同时可以发送数据。

* 依赖哪些条件来判断物理距离的远近的呢？ip, 机架信息嘛？
  * 1. A distance between two nodes can be calculated by summing up their distances to their closest common ancestor. 就是加一下两个节点分别到 common 节点的距离 2. 另外再附赠你一个，HDFS allows an administrator to configure a script that returns a node’s rack identification given a node’s address. The NameNode is the central place that resolves the rack location of each DataNode. When a DataNode registers with the NameNode, the NameNode runs a configured script to decide which rack the node belongs to. If no such a script is configured, the NameNode assumes that all the nodes belong to a default single rack. 管理员可以配置一个脚本用来通过 node 的地址计算 node 的 rack id，这些信息都是通过 NameNode 计算并 resolve 的，如果没有配置脚本，默认所有节点都在一个机架下

* 如果一个操作的耗时主要受网络影响，可尝试聚合多个请求再一起发送。虽然提高了单个请求的延迟，但是提高了整个系统的吞吐量。

* In fact, the client typically asks for multiple chunks in the same request and the master can also include the information for chunks immediately following those requested ？

* prefix compression？ 

* 了解 redis 的 aof 和 rdb 机制

[^1]: 根据论文，大量指数百个



