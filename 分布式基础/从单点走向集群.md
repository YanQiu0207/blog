# 从单点走向集群

（1）性能

问题一：几百个客户端同时访问单点 Master，服务器负载高，请求的延迟很高
解决方法：将数据直接加载到内存，业务直接读写内存，不需要和数据库交互
底层原理：内存 IO 远快于网络 IO 和磁盘 IO

问题二：内存具有易失性，进程挂掉后，数据就丢失了
解决办法：记录内存数据的操作日志。宕机重启后，进程先根据日志恢复到宕机前的状态，再对外提供服务
底层原理：磁盘能够持久化数据。状态机原理，只要输入相同，就能得到同样的输出。

问题三：如果日志量很大，重放日志的方法使得机器的恢复速度很慢
解决办法一：压缩操作日志，有很多日志是冗余的。例如 x=a, x=b, x=c 这三条日志，实际上只需要执行 x=c 就行了。
解决办法二：除了记录操作日志外，还要定时将内存状态刷到磁盘上。宕机重启后，进程先恢复到最近时间点的状态，再重放这个时间点之后的操作日志。
底层原理：减少重复数据

问题四：定时刷盘的方法，仍然可能丢失数据，毕竟数据不是直接实时落盘的（flush）
解决办法：每条操作日志都落盘后才返回成功
底层原理：redo log
额外的问题：性能太低。
思考：没有一种方案可以通吃所有场景。特定场景使用特定方法会有更高的效率。所以，系统设计时要提供多种选择，用户根据使用场景选择合适的使用方式。

问题五：对于读多写少的系统，即使将数据加到内存，性能也还不够
解决办法一：数据在 Master 写入后，异步复制到影子 Master。由于影子 Master 可以有多个，可以分担读的压力。
缺点：影子 Master 的数据可能不是最新的，客户端读到旧数据。一种可能的解决方法是关键数据还是去 Master 读。

（2）可用性

问题一：如果 Master 所在的机器故障，无法启动，数据就丢失了
解决方法：采用同步复制的主从架构。写入数据，Master 和 Backup Master 都写入成功，才给客户端返回成功，保证两者数据始终一致。此时客户端不管是从 master 还是 backup master 读取数据，都是最新的。如果 Master 宕机，可以切换到 Backup Master，不需要担心有数据丢失。
底层原理：数据复制+故障自动转移

问题二：如何知道 master 是否已 crash
解决办法：有个监控程序定时与 Master 通信，如果发现 Master 心跳多次超时，就尝试切换成 Backup Master

问题三：主备切换的步骤
解决办法：
一是激活 backup master，可以接受应用的请求
二是修改 DNS 或其它办法发布新的 master ip，客户端失败后重试时可以连接到新 master

问题四：将备机切换为主机后，就没有备机了，那怎么办？
解决办法：通过增加更多的备库解决，通常是一主两备，既经济又有高可用

问题五：如何保证监控程序的可用性？
解决办法：基于 Paxos/Raft 等共识算法选主

问题六：监控程序无法区分是 Master 宕机还是网络问题造成的心跳超时，有可能造成误切换，导致集群中同时存在两个 Master。如果客户端同时向这两个 Master 写入数据，就可能造成数据丢失和数据混乱的问题。
解决办法：lease 机制，Master 的身份是有时限的。
缺点：可能有一小段时间内不可用

问题七：一旦 backup master 无法同步，整个系统就不可用。
解决办法：部署多个备库来消除网络抖动带来的影响

问题八：如何选择哪个副本？
解决办法：日志最多的